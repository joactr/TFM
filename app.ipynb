{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import face_detection\n",
    "import os, python_speech_features\n",
    "import scipy.io.wavfile as wav\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from talkNet import talkNet\n",
    "from dataset import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import tools\n",
    "detector = face_detection.build_detector(\n",
    "\"DSFDDetector\", confidence_threshold=.3, nms_iou_threshold=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVideo =  \"C:/Users/jmmol/Desktop/LIP-RTVE/MP4s/speaker208/speaker208_0000.mp4\"\n",
    "videoDuration = tools.checkVideoDuration(inputVideo)\n",
    "tools.splitVideo(inputVideo,2,videoDuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga v√≠deo\n",
    "res = tools.saveFaceCrops(inputVideo,detector)\n",
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n",
      "226\n",
      "audio shape:  torch.Size([204, 13])\n",
      "video shape:  torch.Size([51, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# AUDIO PROCESSING\n",
    "audioPath = tools.convert_video_to_audio_ffmpeg(inputVideo)\n",
    "_,sig = wav.read(audioPath)#r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker042_0063.wav\") #\n",
    "#_,sig = wav.read(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker184_0004.wav\") \n",
    "audio = python_speech_features.mfcc(sig, 16000, numcep = 13, winlen = 0.025, winstep = 0.010) #ASUME VIDEO A 25 Y AUDIO A 100, MODIFICAR\n",
    "\n",
    "\n",
    "#TEST\n",
    "#audio = torch.FloatTensor(audio[:180]).unsqueeze(0)\n",
    "#video = torch.FloatTensor(res[:45]).unsqueeze(0)\n",
    "center = 23\n",
    "audio = tools.padAudio(audio,1,center,51,len(res))\n",
    "video = tools.padVideo(res,center,51)\n",
    "print(\"audio shape: \", audio.shape)\n",
    "print(\"video shape: \", video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-23 11:10:19 Model para number = 15.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = talkNet()\n",
    "model.load_state_dict(torch.load(\"./exps/exp2/model/model_0003.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 100\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "datasetTrain = MyDataset(51,videoDir,audioDir,\"testSamples.csv\")\n",
    "item = datasetTrain.__getitem__(1050)\n",
    "valLoader = DataLoader(dataset=datasetTrain,shuffle=False,batch_size=32,num_workers=14) #Cambiar num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m correctSamples, totalSamples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m totalPreds \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m num, (audioFeature, visualFeature, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm\u001b[39m.\u001b[39mtqdm(valLoader)):\n\u001b[0;32m      5\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():    \n\u001b[0;32m      6\u001b[0m         output \u001b[39m=\u001b[39m model((audioFeature,visualFeature))\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correctFrames, totalFrames = 0, 0\n",
    "correctSamples, totalSamples = 0, 0\n",
    "totalPreds = []\n",
    "for num, (audioFeature, visualFeature, labels) in enumerate(tqdm.tqdm(valLoader)):\n",
    "    with torch.no_grad():    \n",
    "        output = model((audioFeature,visualFeature))\n",
    "        labels = labels.cuda()\n",
    "        batchPreds = torch.reshape(output[1], labels.shape)\n",
    "        #Precision a nivel de video\n",
    "        videoPreds = torch.mode(batchPreds,dim=1)[0]\n",
    "        totalPreds.extend(videoPreds.detach().cpu().numpy().astype(int))\n",
    "        labelMode = torch.mode(labels,dim=1)[0]\n",
    "        correctSamples += (videoPreds == labelMode).sum().float()\n",
    "        totalSamples += len(labels)\n",
    "        # Precision a nivel de frame\n",
    "        labels = labels.reshape((-1))\n",
    "        correctFrames += (output[1] == labels).sum().float()\n",
    "        totalFrames += len(labels) \n",
    "#7865\n",
    "print(\"Frame acc:\", correctFrames/totalFrames)\n",
    "print(\"Sample acc:\", correctSamples/totalSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audios iguales? tensor(2652) 2652\n",
      "Videos iguales? tensor(328442) 639744\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Audios iguales?\",(audio == item[0]).sum(),audio.shape[0]*audio.shape[1])\n",
    "print(\"Videos iguales?\",(video == item[1]).sum(),video.shape[0]*112*112)\n",
    "print(audio[0],item[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0040, 0.9960],\n",
      "        [0.0023, 0.9977],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.0026, 0.9974],\n",
      "        [0.0058, 0.9942],\n",
      "        [0.0054, 0.9946],\n",
      "        [0.0031, 0.9969],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.0055, 0.9945],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0085, 0.9915],\n",
      "        [0.0030, 0.9970],\n",
      "        [0.0084, 0.9916],\n",
      "        [0.0070, 0.9930],\n",
      "        [0.0056, 0.9944],\n",
      "        [0.0061, 0.9939],\n",
      "        [0.0083, 0.9917],\n",
      "        [0.0542, 0.9458],\n",
      "        [0.0507, 0.9493],\n",
      "        [0.0072, 0.9928],\n",
      "        [0.0051, 0.9949],\n",
      "        [0.0061, 0.9939],\n",
      "        [0.0094, 0.9906],\n",
      "        [0.0171, 0.9829],\n",
      "        [0.0101, 0.9899],\n",
      "        [0.0037, 0.9963],\n",
      "        [0.0129, 0.9871],\n",
      "        [0.0034, 0.9966],\n",
      "        [0.0345, 0.9655],\n",
      "        [0.0248, 0.9752],\n",
      "        [0.0069, 0.9931],\n",
      "        [0.0077, 0.9923],\n",
      "        [0.0058, 0.9942],\n",
      "        [0.0120, 0.9880],\n",
      "        [0.0121, 0.9879],\n",
      "        [0.0026, 0.9974],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.0014, 0.9986],\n",
      "        [0.0042, 0.9958],\n",
      "        [0.0034, 0.9966],\n",
      "        [0.0035, 0.9965],\n",
      "        [0.0030, 0.9970],\n",
      "        [0.0038, 0.9962],\n",
      "        [0.0045, 0.9955],\n",
      "        [0.0018, 0.9982],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.0023, 0.9977],\n",
      "        [0.0160, 0.9840],\n",
      "        [0.0133, 0.9867],\n",
      "        [0.0095, 0.9905]], device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0'))\n",
      "Pred -> tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "iAudio = audio.unsqueeze(0)\n",
    "iVideo = video.unsqueeze(0)\n",
    "# iAudio = item[0].unsqueeze(0)\n",
    "# iVideo = item[1].unsqueeze(0)\n",
    "output = model((iAudio,iVideo))\n",
    "#output = model((audio,video))\n",
    "print(output)\n",
    "print(\"Pred ->\",torch.mode(output[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0            video            audio  label  center  Prediction\n",
      "0               0  speaker073_0000  speaker073_0000      1      34           1\n",
      "1               1  speaker205_0000  speaker205_0001      0      75           0\n",
      "2               2  speaker163_0000  speaker163_0000      1     118           1\n",
      "3               3  speaker314_0011  speaker314_0011      1      18           1\n",
      "4               4  speaker300_0001  speaker300_0001      0      19           0\n",
      "...           ...              ...              ...    ...     ...         ...\n",
      "29996       29996  speaker310_0006  speaker310_0006      1      59           1\n",
      "29997       29997  speaker169_0002  speaker169_0002      1      50           1\n",
      "29998       29998  speaker291_0000  speaker155_0006      0     143           0\n",
      "29999       29999  speaker258_0000  speaker197_0005      0      96           0\n",
      "30000       30000  speaker065_0025  speaker065_0025      1      25           1\n",
      "\n",
      "[30001 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(totalPreds)\n",
    "dfPreds = pd.DataFrame(totalPreds,columns=[\"Prediction\"])\n",
    "dfSamples = pd.read_csv(\"testSamples.csv\")\n",
    "dfRes = dfSamples.join(dfPreds)\n",
    "print(dfRes)\n",
    "dfRes.to_csv(\"testPreds5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv(\"trainSamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfTrain\u001b[39m.\u001b[39;49mjoin(dfSamples,on\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvideo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\pandas\\core\\frame.py:9729\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m   9566\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\n\u001b[0;32m   9567\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9568\u001b[0m     other: DataFrame \u001b[39m|\u001b[39m Series \u001b[39m|\u001b[39m Iterable[DataFrame \u001b[39m|\u001b[39m Series],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9574\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   9575\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   9576\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   9577\u001b[0m \u001b[39m    Join columns of another DataFrame.\u001b[39;00m\n\u001b[0;32m   9578\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9727\u001b[0m \u001b[39m    5  K1  A5   B1\u001b[39;00m\n\u001b[0;32m   9728\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 9729\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_join_compat(\n\u001b[0;32m   9730\u001b[0m         other,\n\u001b[0;32m   9731\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m   9732\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m   9733\u001b[0m         lsuffix\u001b[39m=\u001b[39;49mlsuffix,\n\u001b[0;32m   9734\u001b[0m         rsuffix\u001b[39m=\u001b[39;49mrsuffix,\n\u001b[0;32m   9735\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9736\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   9737\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\pandas\\core\\frame.py:9768\u001b[0m, in \u001b[0;36mDataFrame._join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m   9758\u001b[0m     \u001b[39mif\u001b[39;00m how \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   9759\u001b[0m         \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m   9760\u001b[0m             \u001b[39mself\u001b[39m,\n\u001b[0;32m   9761\u001b[0m             other,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9766\u001b[0m             validate\u001b[39m=\u001b[39mvalidate,\n\u001b[0;32m   9767\u001b[0m         )\n\u001b[1;32m-> 9768\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m   9769\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   9770\u001b[0m         other,\n\u001b[0;32m   9771\u001b[0m         left_on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m   9772\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m   9773\u001b[0m         left_index\u001b[39m=\u001b[39;49mon \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   9774\u001b[0m         right_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   9775\u001b[0m         suffixes\u001b[39m=\u001b[39;49m(lsuffix, rsuffix),\n\u001b[0;32m   9776\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9777\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   9778\u001b[0m     )\n\u001b[0;32m   9779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   9780\u001b[0m     \u001b[39mif\u001b[39;00m on \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:142\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 142\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    143\u001b[0m         left,\n\u001b[0;32m    144\u001b[0m         right,\n\u001b[0;32m    145\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    146\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    147\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    148\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    149\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    150\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    151\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    152\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    153\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    154\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:735\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    727\u001b[0m (\n\u001b[0;32m    728\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    729\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    730\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m    731\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    733\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[0;32m    737\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1387\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1381\u001b[0m     \u001b[39m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[0;32m   1383\u001b[0m         inferred_left \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_right \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1384\u001b[0m     ) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1385\u001b[0m         inferred_right \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_left \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[0;32m   1386\u001b[0m     ):\n\u001b[1;32m-> 1387\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1389\u001b[0m \u001b[39m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "dfTrain.join(dfSamples,on=\"video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
