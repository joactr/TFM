{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_detection\n",
    "import collections\n",
    "import os, python_speech_features\n",
    "import scipy.io.wavfile as wav\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import subprocess\n",
    "import torch\n",
    "from talkNet import talkNet\n",
    "from dataset import MyDataset\n",
    "detector = face_detection.build_detector(\n",
    "  \"DSFDDetector\", confidence_threshold=.2, nms_iou_threshold=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVideo =  \"C:/Users/jmmol/Desktop/LIP-RTVE/MP4s/speaker042/speaker042_0007.mp4\"\n",
    "\n",
    "def convert_video_to_audio_ffmpeg(video_file, output_ext=\"wav\"):\n",
    "    \"\"\"Extracts audio from video to .wav format, returns path of the resulting audio\"\"\"\n",
    "    filename, ext = os.path.splitext(video_file)\n",
    "    filename = filename.split('/')[-1]\n",
    "    #Gets working directory and extracts audio\n",
    "    subprocess.call([\"ffmpeg\", \"-y\", \"-i\", video_file, os.getcwd()+f\"/{filename}.{output_ext}\"],\n",
    "                    stdout=subprocess.DEVNULL,\n",
    "                    stderr=subprocess.STDOUT)\n",
    "    return os.getcwd()+f\"/{filename}.{output_ext}\" #Nombre del audio\n",
    "\n",
    "def extractBiggestFace(img):\n",
    "    \"\"\"\n",
    "    Detecta todas las caras de una imagen y devuelve la más grande recortada y reescalada a 112x112\n",
    "    \"\"\"\n",
    "    detections = detector.detect(img)\n",
    "    idx_max = -1\n",
    "    area_max = -1\n",
    "    for i,cntr in enumerate(detections):\n",
    "        xmin,ymin,xmax,ymax = int(cntr[0]),int(cntr[1]),int(cntr[2]),int(cntr[3]) #Guardamos bounding box\n",
    "        area = (xmax-xmin)*(ymax-ymin)\n",
    "        if area > area_max: #Comprobamos si la cara es la más grande\n",
    "            idx_max = i\n",
    "            area_max = area\n",
    "            #print(area,idx_max)\n",
    "        #cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "\n",
    "    cntr = detections[idx_max]\n",
    "    try:\n",
    "        xmin,ymin,xmax,ymax = int(cntr[0]),int(cntr[1]),int(cntr[2]),int(cntr[3])\n",
    "        resImage = cv2.resize(img[max(ymin,0):ymax, xmin:xmax], (112, 112)) #Cara detectada, reescalamos\n",
    "        resImage = cv2.cvtColor(resImage, cv2.COLOR_BGR2GRAY)\n",
    "        return resImage\n",
    "    except:\n",
    "        print(cntr)\n",
    "        cv2.imshow('image',img)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "def saveFaceCrops(videoPath):\n",
    "    vidcap = cv2.VideoCapture(videoPath)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    faceArray = []\n",
    "    while success:\n",
    "        faceArray.append(extractBiggestFace(image)) #DE MOMENTO SIEMPRE HAY CARA\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "    return faceArray #Devuelve número de frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = saveFaceCrops(inputVideo)\n",
    "res[0].shape\n",
    "\n",
    "# cv2.imshow('image',res[3])\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio shape:  torch.Size([1, 28, 13])\n",
      "video shape:  torch.Size([1, 7, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# AUDIO PROCESSING\n",
    "audioPath = convert_video_to_audio_ffmpeg(inputVideo)\n",
    "_,sig = wav.read(audioPath)#r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker042_0063.wav\") #\n",
    "_,sig = wav.read(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker042_0004.wav\") \n",
    "audio = python_speech_features.mfcc(sig, 16000, numcep = 13, winlen = 0.025, winstep = 0.010) #ASUME VIDEO A 25 Y AUDIO A 100, MODIFICAR\n",
    "\n",
    "\n",
    "#TEST\n",
    "audio = torch.FloatTensor(audio[:28]).unsqueeze(0)\n",
    "video = torch.FloatTensor(res[:7]).unsqueeze(0)\n",
    "print(\"audio shape: \", audio.shape)\n",
    "print(\"video shape: \", video.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 100\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "datasetTrain = MyDataset(51,videoDir,audioDir,\"trainSamples.csv\")\n",
    "item = datasetTrain.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-19 19:11:35 Model para number = 15.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = talkNet()\n",
    "model.load_state_dict(torch.load(\"./exps/exp2/model/model_0012.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 13]) torch.Size([1, 7, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "#output = model((item[0].unsqueeze(0),item[1].unsqueeze(0)))\n",
    "output = model((audio,video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2311, device='cuda:0'),\n",
       " tensor([[0.2008, 0.7992],\n",
       "         [0.2021, 0.7979],\n",
       "         [0.2508, 0.7492],\n",
       "         [0.1506, 0.8494],\n",
       "         [0.1983, 0.8017],\n",
       "         [0.2033, 0.7967],\n",
       "         [0.2349, 0.7651]], device='cuda:0'),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:0'),\n",
       " tensor(7., device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49margmax(output, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
