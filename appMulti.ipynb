{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import face_detection\n",
    "import os, python_speech_features\n",
    "import scipy.io.wavfile as wav\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from talkNet import talkNet\n",
    "from dataset import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "import pickle\n",
    "import tools\n",
    "import sys\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import whisper\n",
    "module_path = os.path.abspath(os.path.join('./light'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from light.ASD import ASD as lightASD\n",
    "detector = face_detection.build_detector(\n",
    "\"DSFDDetector\", confidence_threshold=.3, nms_iou_threshold=.5) #DSFDDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVideo = \"C:/Users/jmmol/Desktop/LIP-RTVE/MP4s/speaker092/speaker092_0002.mp4\"\n",
    "#inputVideo = r\"inputs/dos_personas_hablando_por_turnos.mp4\"\n",
    "videoDuration = tools.checkVideoDuration(inputVideo)\n",
    "videoFrames = int(videoDuration*25)\n",
    "fps = 25\n",
    "#tools.splitVideo(inputVideo,2,videoDuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº CARAS: 4\n",
      "266\n",
      "266\n",
      "271\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "#Carga vídeo\n",
    "res, facePos, faceFrames = tools.saveMultiFace(inputVideo,detector,50)\n",
    "# AUDIO PROCESSING\n",
    "audioPath = tools.convert_video_to_audio_ffmpeg(inputVideo)\n",
    "_,sig = wav.read(audioPath)#r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker042_0063.wav\") #\n",
    "# _,sig = wav.read(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker000_0000.wav\") \n",
    "# audioPath = r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker000_0000.wav\"\n",
    "audio = python_speech_features.mfcc(sig, 16000, numcep = 13, winlen = 0.025, winstep = 0.010) #ASUME VIDEO A 25 Y AUDIO A 100, MODIFICAR\n",
    "print(\"Nº CARAS:\",len(res))\n",
    "for cara in res.keys():\n",
    "    print(len(res[cara]))\n",
    "# cv2.imshow(\"My Video\", res[3][50])\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-05 20:01:25 Model para number = 15.01\n"
     ]
    }
   ],
   "source": [
    "model = talkNet()\n",
    "#model.load_state_dict(torch.load(\"./exps/exp2/model/model_0002.model\"))\n",
    "model.load_state_dict(torch.load(\"./exps/exp1/model/model13_0006.model\"))\n",
    "#model.load_state_dict(torch.load(\"./exps/exp1/model/model21_0006.model\"))\n",
    "\n",
    "#model.load_state_dict(torch.load(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\exps\\exp1\\model\\model_01.model\"))\n",
    "windowSize = 13\n",
    "\n",
    "# ASD = lightASD()\n",
    "# ASD.loadParameters(\"./light/weight/pretrain_AVA_CVPR.model\")\n",
    "# model = ASD.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 24\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "datasetTrain = MyDataset(windowSize,videoDir,audioDir,\"testSamples.csv\")\n",
    "item = datasetTrain.__getitem__(0)\n",
    "valLoader = DataLoader(dataset=datasetTrain,shuffle=False,batch_size=8,num_workers=14) #Cambiar num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3751/3751 [02:03<00:00, 30.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame acc: tensor(0.7626, device='cuda:0')\n",
      "Sample acc: tensor(0.7632, device='cuda:0')\n",
      "AUC: 0.8914382501742111\n"
     ]
    }
   ],
   "source": [
    "correctFrames, totalFrames = 0, 0\n",
    "correctSamples, totalSamples = 0, 0\n",
    "totalPreds = []\n",
    "totalScores = []\n",
    "for num, (audioFeature, visualFeature, labels) in enumerate(tqdm.tqdm(valLoader)):\n",
    "    with torch.no_grad():    \n",
    "        audioFeature, visualFeature, labels = audioFeature.cuda(), visualFeature.cuda(), labels.cuda()\n",
    "        predScores,predLabels = model((audioFeature,visualFeature)) #(audioFeature,visualFeature) si talknet\n",
    "        batchPreds = torch.reshape(predLabels, labels.shape)\n",
    "        #Precision a nivel de video\n",
    "        videoPreds = torch.mode(batchPreds,dim=1)[0]\n",
    "        totalPreds.extend(videoPreds.detach().cpu().numpy().astype(int))\n",
    "        for ten in torch.split(predScores[:,1],windowSize):\n",
    "            totalScores.append(torch.mean(ten).detach().cpu().numpy())\n",
    "        labelMode = torch.mode(labels,dim=1)[0]\n",
    "        correctSamples += (videoPreds == labelMode).sum().float()\n",
    "        totalSamples += len(labels)\n",
    "        # Precision a nivel de frame\n",
    "        labels = labels.reshape((-1))\n",
    "        correctFrames += (predLabels == labels).sum().float()\n",
    "        totalFrames += len(labels) \n",
    "#7865\n",
    "print(\"Frame acc:\", correctFrames/totalFrames)\n",
    "print(\"Sample acc:\", correctSamples/totalSamples)\n",
    "# center = 47\n",
    "# iAudio = tools.padAudio(audio,1,center,51,len(res[0])).unsqueeze(0)\n",
    "# iVideo = tools.padVideo(res[0],center,51).unsqueeze(0)\n",
    "# print(iAudio[40],item[0][40])\n",
    "# print(\"Audios iguales?\",(iAudio == item[0]).sum(),iAudio.shape[0]*iAudio.shape[1])\n",
    "# print(\"Videos iguales?\",(iVideo == item[1]).sum(),iVideo.shape[0]*112*112)\n",
    "# print(audio[0],item[0][0])\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv(\"testSamples.csv\")\n",
    "df2 = pd.DataFrame({'pred':totalPreds})\n",
    "df3 = pd.DataFrame({'posScore':totalScores})\n",
    "df[\"pred\"] = df2\n",
    "df[\"posScore\"] = df3\n",
    "df.to_csv(\"testPredsX.csv\")\n",
    "g_truth = pd.read_csv(\"testSamples.csv\")[\"label\"].values.tolist()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(g_truth, totalScores)\n",
    "auc = metrics.roc_auc_score(g_truth, totalScores)\n",
    "np.savetxt('cli.txt', df[df[\"label\"]==1][\"posScore\"].values, fmt='%f')\n",
    "np.savetxt('imp.txt', df[df[\"label\"]==0][\"posScore\"].values, fmt='%f')\n",
    "nCorrect = 0\n",
    "for i in df[df[\"label\"]==1][\"posScore\"].values:\n",
    "    if i >=0.5:\n",
    "        nCorrect +=1\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sidewindow: 10\n"
     ]
    }
   ],
   "source": [
    "totalScores = defaultdict(list)\n",
    "sideWindowSize = int((windowSize-1)/2)\n",
    "print(\"sidewindow:\",sideWindowSize)\n",
    "sequential = True\n",
    "meanWSize = 11\n",
    "meanSideSize = int((meanWSize-1)/2)\n",
    "thr = 0.190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m sequential:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# SECUENCIAL\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m actualSpeaker \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSPEAKER\u001b[39m\u001b[39m\"\u001b[39m, actualSpeaker)\n\u001b[0;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(res[actualSpeaker])\u001b[39m+\u001b[39mwindowSize\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,windowSize):\n\u001b[0;32m      6\u001b[0m             \u001b[39m#center = i\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "if sequential:\n",
    "    # SECUENCIAL\n",
    "    for actualSpeaker in res.keys():\n",
    "        print(\"SPEAKER\", actualSpeaker)\n",
    "        for i in range(0,len(res[actualSpeaker])+windowSize+1,windowSize):\n",
    "            #center = i\n",
    "            center = faceFrames[actualSpeaker][0]+i\n",
    "            #print(\"FRAME Nº \",center)\n",
    "            iAudio = tools.padAudio(audio,1,center,windowSize,videoFrames).unsqueeze(0)\n",
    "            iVideo = tools.padVideo(res[actualSpeaker],center-faceFrames[actualSpeaker][0],windowSize).unsqueeze(0)\n",
    "            #print(iAudio[0][0][0:5],iVideo[0][0][0][0:5])\n",
    "            scores,labels= model((iAudio,iVideo))\n",
    "            totalScores[actualSpeaker].extend(scores[:,1].detach().cpu().numpy().tolist())\n",
    "            #predArray[actualSpeaker].extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "        # MEAN SLIDING WINDOW\n",
    "        for i in range(len(totalScores[actualSpeaker])):\n",
    "            ini = max(0,i-meanSideSize) # No negative values\n",
    "            end = i+meanSideSize+1 # +1 as python does not take into account last value\n",
    "            #print(ini,end)\n",
    "            totalScores[actualSpeaker][i] = np.mean(totalScores[actualSpeaker][ini:end])\n",
    "        print(totalScores[actualSpeaker])\n",
    "else:\n",
    "    # MINIMO\n",
    "    for actualSpeaker in res.keys():\n",
    "        print(\"SPEAKER\", actualSpeaker, \"LENGTH:\", len(res[actualSpeaker]))\n",
    "        for i in range(len(res[actualSpeaker])):\n",
    "            #center = i\n",
    "            center = faceFrames[actualSpeaker][0]+i\n",
    "            print(\"FRAME Nº \",center)\n",
    "            iAudio = tools.padAudio(audio,1,center,windowSize,videoFrames).unsqueeze(0)\n",
    "            iVideo = tools.padVideo(res[actualSpeaker],center-faceFrames[actualSpeaker][0],windowSize).unsqueeze(0)\n",
    "            #print(iAudio[0][0][0:5],iVideo[0][0][0][0:5])\n",
    "            scores,labels= model((iAudio,iVideo))\n",
    "            print(\"\\tProb Pos ->\",scores[sideWindowSize][1])\n",
    "            totalScores[actualSpeaker].append(scores[sideWindowSize][1].detach().cpu().numpy())\n",
    "        \n",
    "        # MEAN SLIDING WINDOW\n",
    "        for i in range(len(totalScores[actualSpeaker])):\n",
    "            ini = max(0,i-meanSideSize) # No negative values\n",
    "            end = i+meanSideSize+1 # +1 as python does not take into account last value\n",
    "            #print(ini,end)\n",
    "            totalScores[actualSpeaker][i] = np.mean(totalScores[actualSpeaker][ini:end])\n",
    "        print(totalScores[actualSpeaker])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "266\n",
      "271\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# SMOOTHING\n",
    "predArray = defaultdict(list)\n",
    "\n",
    "# APPLY THRESHOLD\n",
    "for actualSpeaker in res.keys():\n",
    "    # Delete < 0 frame predictions (initial)\n",
    "    #Solo para metodo secuencial ->\n",
    "    if sequential:\n",
    "        totalScores[actualSpeaker] = totalScores[actualSpeaker][sideWindowSize:len(res[actualSpeaker])+sideWindowSize]\n",
    "    # Classify as speaking if score higher than threshold\n",
    "    for sc in totalScores[actualSpeaker]:\n",
    "        predLabel = 1 if sc > thr else 0\n",
    "        predArray[actualSpeaker].append(predLabel)\n",
    "\n",
    "    print(len(predArray[actualSpeaker]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVideo(videoName,imgFrames,audioPath,width,height):\n",
    "    video = cv2.VideoWriter(\"output.mp4\", 0, 25, (width,height))\n",
    "\n",
    "    for image in imgFrames:\n",
    "        video.write(image)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    res = subprocess.check_output([\"ffmpeg\",\"-y\",\"-i\",os.getcwd()+f\"/output.mp4\",\"-i\",audioPath,\"-map\",\"0:v\",\"-map\",\"1:a\", \"-c:v\", \"copy\", \"-shortest\", os.getcwd()+f\"/outputs/videos/{videoName}\"])\n",
    "    os.remove(os.getcwd()+f\"/output.mp4\")\n",
    "    \n",
    "    print(res)\n",
    "\n",
    "# Returns a list of [start,end] timestamps in seconds of the parts where a speaker is speaking with minimum length of minLength frames\n",
    "def getSpeaking(arr,minLength,fps):\n",
    "    prev_idx = 0\n",
    "    posFrames = 0\n",
    "    idx_list = []\n",
    "    for i,num in enumerate(arr):\n",
    "        if num == 1:\n",
    "            posFrames +=1\n",
    "        else:\n",
    "            if i-prev_idx >= minLength:\n",
    "                idx_list.append((prev_idx/fps,i/fps))\n",
    "            prev_idx = i\n",
    "            posFrames = 0\n",
    "    return idx_list\n",
    "\n",
    "def saveFullVideo(videoName):\n",
    "    cap = cv2.VideoCapture(inputVideo)\n",
    "    videoImages = []\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    while True:\n",
    "        ret, image = cap.read()\n",
    "        videoImages.append(image)\n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "\n",
    "    for speakerN in totalScores.keys():\n",
    "        for i, fr in enumerate(faceFrames[speakerN]): #Para cada frame en el que aparezca la cara\n",
    "            try:\n",
    "                image = videoImages[fr]\n",
    "                greenValue = int(255*predArray[speakerN][i])\n",
    "                redValue = 255-greenValue\n",
    "                color = (0,greenValue,redValue)\n",
    "                #print(color)\n",
    "                xmin,ymin,xmax,ymax = facePos[speakerN][i]\n",
    "                image = cv2.rectangle(image, (xmin,ymin), (xmax,ymax),color , 1)\n",
    "                cv2.putText(image, \"{:.2f}\".format(totalScores[speakerN][i]*100), (xmin, ymin-5), cv2.FONT_HERSHEY_SIMPLEX, 0.35, color, 1)\n",
    "            except:\n",
    "                pass\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    createVideo(videoName,videoImages,audioPath,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "#Asignación de bounding boxes y probabilidades al video de output\n",
    "os.makedirs(\"outputs/videos\", exist_ok=True)\n",
    "path = os.path.normpath(inputVideo)\n",
    "videoName = str(path.split(os.sep)[-1])\n",
    "saveFullVideo(videoName)\n",
    "\n",
    "#print(predArray[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\whisper\\timing.py:42: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower median kernel implementation...\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\whisper\\timing.py:146: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower DTW implementation...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:08.340]  es que había una estrategia diseñada de realizar un golpe de Estado institucional al margen de la legalidad.\n",
      "[00:08.500 --> 00:10.140]  Y alguien deberá responder por ello.\n"
     ]
    }
   ],
   "source": [
    "# GUARDAR DATOS \n",
    "# COGER SOLO FRAGMENTO DE AUDIO EN EL QUE SE DETECTA HABLANTE\n",
    "os.makedirs(\"outputs/pickles\", exist_ok=True)\n",
    "model = whisper.load_model(\"small\")\n",
    "transcription = model.transcribe(audioPath, language=\"es\", verbose=True,word_timestamps=True)\n",
    "pkDict = {\"facePos\":facePos,\"faceFrames\":faceFrames,\"preds\":predArray,\"transcription\":transcription}\n",
    "with open(\"outputs/npz/\"+videoName+'.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(pkDict, f) # serialize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRAER PARTE APROPIADA DE LA TRANSCRIPCIÓN\n",
    "wordArr = []\n",
    "alignArr = []\n",
    "for seg in transcription[\"segments\"]:\n",
    "    for w in seg[\"words\"]:\n",
    "        wordArr.append(w[\"word\"])\n",
    "        alignArr.append((w[\"start\"],w[\"end\"]))\n",
    "print(wordArr)\n",
    "print(alignArr)\n",
    "pdRows = []\n",
    "\n",
    "for speakerN in totalScores.keys():\n",
    "    for (ini,end) in getSpeaking(predArray[speakerN],5,25):\n",
    "        iniW, endW = -1, -1\n",
    "        for i, (alignIni, alignEnd) in enumerate(alignArr):\n",
    "            if iniW == -1 and ini <= alignEnd and ini >= alignIni:\n",
    "                iniW = i\n",
    "            if endW == -1 and end <= alignEnd and end >= alignIni:\n",
    "                endW = i\n",
    "        if iniW > -1 and endW > -1:\n",
    "            newRow = {'video':inputVideo, 'speaker':speakerN, 'ini': ini, 'end':end, 'dataPath': \"outputs/npz/\"+videoName+\".pkl\", 'transcription':''.join(wordArr[iniW:endW+1])}\n",
    "            pdRows.append(newRow)\n",
    "\n",
    "df = pd.DataFrame(pdRows,columns=['video', 'speaker', 'ini', 'end', 'dataPath', 'transcription'])\n",
    "df.to_csv(r\"outputs/res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7570406648962766\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv(\"testSamples.csv\")\n",
    "df2 = pd.DataFrame({'pred':totalPreds})\n",
    "df3 = pd.DataFrame({'posScore':totalScores})\n",
    "df[\"pred\"] = df2\n",
    "df[\"posScore\"] = df3\n",
    "df.to_csv(\"testPredsX.csv\")\n",
    "g_truth = pd.read_csv(\"testSamples.csv\")[\"label\"].values.tolist()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(g_truth, totalScores)\n",
    "auc = metrics.roc_auc_score(g_truth, totalScores)\n",
    "np.savetxt('cli.txt', df[df[\"label\"]==1][\"posScore\"].values, fmt='%f')\n",
    "np.savetxt('imp.txt', df[df[\"label\"]==0][\"posScore\"].values, fmt='%f')\n",
    "nCorrect = 0\n",
    "for i in df[df[\"label\"]==1][\"posScore\"].values:\n",
    "    if i >=0.5:\n",
    "        nCorrect +=1\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.002, F-Score=0.79077\n"
     ]
    }
   ],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    " return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "probs = df[\"posScore\"].values.tolist()\n",
    "test_y = df[\"label\"].values.tolist()\n",
    "\n",
    "scores = [f1_score(test_y, to_labels(probs, t)) for t in thresholds]\n",
    "ix = np.argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
