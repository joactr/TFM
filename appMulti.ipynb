{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import face_detection\n",
    "import os, python_speech_features\n",
    "import scipy.io.wavfile as wav\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from talkNet import talkNet\n",
    "from dataset import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "import pickle\n",
    "import tools\n",
    "import sys\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import whisper\n",
    "module_path = os.path.abspath(os.path.join('./light'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from light.ASD import ASD as lightASD\n",
    "detector = face_detection.build_detector(\n",
    "\"DSFDDetector\", confidence_threshold=.3, nms_iou_threshold=.5) #DSFDDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVideo = \"C:/Users/jmmol/Desktop/LIP-RTVE/MP4s/speaker092/speaker092_0002.mp4\"\n",
    "#inputVideo = r\"inputs/dos_personas_hablando_por_turnos.mp4\"\n",
    "videoDuration = tools.checkVideoDuration(inputVideo)\n",
    "videoFrames = int(videoDuration*25)\n",
    "fps = 25\n",
    "#tools.splitVideo(inputVideo,2,videoDuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº CARAS: 4\n",
      "266\n",
      "266\n",
      "271\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "#Carga vídeo\n",
    "res, facePos, faceFrames = tools.saveMultiFace(inputVideo,detector,50)\n",
    "# AUDIO PROCESSING\n",
    "audioPath = tools.convert_video_to_audio_ffmpeg(inputVideo)\n",
    "_,sig = wav.read(audioPath)#r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker042_0063.wav\") #\n",
    "# _,sig = wav.read(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker000_0000.wav\") \n",
    "# audioPath = r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\speaker000_0000.wav\"\n",
    "audio = python_speech_features.mfcc(sig, 16000, numcep = 13, winlen = 0.025, winstep = 0.010) #ASUME VIDEO A 25 Y AUDIO A 100, MODIFICAR\n",
    "print(\"Nº CARAS:\",len(res))\n",
    "for cara in res.keys():\n",
    "    print(len(res[cara]))\n",
    "# cv2.imshow(\"My Video\", res[3][50])\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-23 19:06:45 Model para number = 15.01\n"
     ]
    }
   ],
   "source": [
    "model = talkNet()\n",
    "#model.load_state_dict(torch.load(\"./exps/exp2/model/model_0002.model\"))\n",
    "model.load_state_dict(torch.load(\"./exps/exp1/model/model21_0006.model\"))\n",
    "windowSize = 51\n",
    "\n",
    "# ASD = lightASD()\n",
    "# ASD.loadParameters(\"./light/weight/pretrain_AVA_CVPR.model\")\n",
    "# model = ASD.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "datasetTrain = MyDataset(windowSize,videoDir,audioDir,\"testSamples.csv\")\n",
    "item = datasetTrain.__getitem__(0)\n",
    "valLoader = DataLoader(dataset=datasetTrain,shuffle=False,batch_size=8,num_workers=14) #Cambiar num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 508\n",
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 292/5000 [00:05<01:29, 52.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m audioFeature, visualFeature, labels \u001b[39m=\u001b[39m audioFeature\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), visualFeature\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m)):\n\u001b[1;32m----> 8\u001b[0m     predScores,predLabels \u001b[39m=\u001b[39m model((audioFeature,visualFeature))\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\talkNet.py:27\u001b[0m, in \u001b[0;36mtalkNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m#print(audioFeature.shape,visualFeature.shape)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m audioEmbed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward_audio_frontend(audioFeature\u001b[39m.\u001b[39mcuda()) \u001b[39m# feedForward\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m visualEmbed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_visual_frontend(visualFeature\u001b[39m.\u001b[39;49mcuda())\n\u001b[0;32m     28\u001b[0m audioEmbed, visualEmbed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward_cross_attention(audioEmbed, visualEmbed)\n\u001b[0;32m     29\u001b[0m outsAV\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward_audio_visual_backend(audioEmbed, visualEmbed)  \n",
      "File \u001b[1;32mc:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\model\\talkNetModel.py:36\u001b[0m, in \u001b[0;36mtalkNetModel.forward_visual_frontend\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, T, \u001b[39m512\u001b[39m)        \n\u001b[0;32m     35\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)     \n\u001b[1;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisualTCN(x)\n\u001b[0;32m     37\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualConv1D(x)\n\u001b[0;32m     38\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\model\\visualEncoder.py:157\u001b[0m, in \u001b[0;36mvisualTCN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 157\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\model\\visualEncoder.py:145\u001b[0m, in \u001b[0;36mDSConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 145\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n\u001b[0;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m out \u001b[39m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked\u001b[39m.\u001b[39;49madd_(\u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    153\u001b[0m             exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "windowSize = 256\n",
    "datasetTrain = MyDataset(windowSize,videoDir,audioDir,\"testSamples.csv\")\n",
    "item = datasetTrain.__getitem__(0)\n",
    "audioFeature, visualFeature, labels = item\n",
    "print(len(visualFeature))\n",
    "audioFeature, visualFeature, labels = audioFeature.cuda().unsqueeze(0), visualFeature.cuda().unsqueeze(0), labels.cuda()\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    predScores,predLabels = model((audioFeature,visualFeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3751 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3751/3751 [02:01<00:00, 30.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame acc: tensor(0.7634, device='cuda:0')\n",
      "Sample acc: tensor(0.7638, device='cuda:0')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 40 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m iAudio \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39mpadAudio(audio,\u001b[39m1\u001b[39m,center,\u001b[39m51\u001b[39m,\u001b[39mlen\u001b[39m(res[\u001b[39m0\u001b[39m]))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m iVideo \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39mpadVideo(res[\u001b[39m0\u001b[39m],center,\u001b[39m51\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(iAudio[\u001b[39m40\u001b[39;49m],item[\u001b[39m0\u001b[39m][\u001b[39m40\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAudios iguales?\u001b[39m\u001b[39m\"\u001b[39m,(iAudio \u001b[39m==\u001b[39m item[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39msum(),iAudio\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39miAudio\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVideos iguales?\u001b[39m\u001b[39m\"\u001b[39m,(iVideo \u001b[39m==\u001b[39m item[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39msum(),iVideo\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m112\u001b[39m\u001b[39m*\u001b[39m\u001b[39m112\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 40 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "correctFrames, totalFrames = 0, 0\n",
    "correctSamples, totalSamples = 0, 0\n",
    "totalPreds = []\n",
    "totalScores = []\n",
    "for num, (audioFeature, visualFeature, labels) in enumerate(tqdm.tqdm(valLoader)):\n",
    "    with torch.no_grad():    \n",
    "        audioFeature, visualFeature, labels = audioFeature.cuda(), visualFeature.cuda(), labels.cuda()\n",
    "        predScores,predLabels = model((audioFeature,visualFeature)) #(audioFeature,visualFeature) si talknet\n",
    "        batchPreds = torch.reshape(predLabels, labels.shape)\n",
    "        #Precision a nivel de video\n",
    "        videoPreds = torch.mode(batchPreds,dim=1)[0]\n",
    "        totalPreds.extend(videoPreds.detach().cpu().numpy().astype(int))\n",
    "        for ten in torch.split(predScores[:,1],windowSize):\n",
    "            totalScores.append(torch.mean(ten).detach().cpu().numpy())\n",
    "        labelMode = torch.mode(labels,dim=1)[0]\n",
    "        correctSamples += (videoPreds == labelMode).sum().float()\n",
    "        totalSamples += len(labels)\n",
    "        # Precision a nivel de frame\n",
    "        labels = labels.reshape((-1))\n",
    "        correctFrames += (predLabels == labels).sum().float()\n",
    "        totalFrames += len(labels) \n",
    "#7865\n",
    "print(\"Frame acc:\", correctFrames/totalFrames)\n",
    "print(\"Sample acc:\", correctSamples/totalSamples)\n",
    "center = 47\n",
    "iAudio = tools.padAudio(audio,1,center,51,len(res[0])).unsqueeze(0)\n",
    "iVideo = tools.padVideo(res[0],center,51).unsqueeze(0)\n",
    "print(iAudio[40],item[0][40])\n",
    "print(\"Audios iguales?\",(iAudio == item[0]).sum(),iAudio.shape[0]*iAudio.shape[1])\n",
    "print(\"Videos iguales?\",(iVideo == item[1]).sum(),iVideo.shape[0]*112*112)\n",
    "print(audio[0],item[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sidewindow: 10\n"
     ]
    }
   ],
   "source": [
    "totalScores = defaultdict(list)\n",
    "sideWindowSize = int((windowSize-1)/2)\n",
    "print(\"sidewindow:\",sideWindowSize)\n",
    "sequential = True\n",
    "meanWSize = 11\n",
    "meanSideSize = int((meanWSize-1)/2)\n",
    "thr = 0.190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m sequential:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# SECUENCIAL\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m actualSpeaker \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSPEAKER\u001b[39m\u001b[39m\"\u001b[39m, actualSpeaker)\n\u001b[0;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(res[actualSpeaker])\u001b[39m+\u001b[39mwindowSize\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,windowSize):\n\u001b[0;32m      6\u001b[0m             \u001b[39m#center = i\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "if sequential:\n",
    "    # SECUENCIAL\n",
    "    for actualSpeaker in res.keys():\n",
    "        print(\"SPEAKER\", actualSpeaker)\n",
    "        for i in range(0,len(res[actualSpeaker])+windowSize+1,windowSize):\n",
    "            #center = i\n",
    "            center = faceFrames[actualSpeaker][0]+i\n",
    "            #print(\"FRAME Nº \",center)\n",
    "            iAudio = tools.padAudio(audio,1,center,windowSize,videoFrames).unsqueeze(0)\n",
    "            iVideo = tools.padVideo(res[actualSpeaker],center-faceFrames[actualSpeaker][0],windowSize).unsqueeze(0)\n",
    "            #print(iAudio[0][0][0:5],iVideo[0][0][0][0:5])\n",
    "            scores,labels= model((iAudio,iVideo))\n",
    "            totalScores[actualSpeaker].extend(scores[:,1].detach().cpu().numpy().tolist())\n",
    "            #predArray[actualSpeaker].extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "        # MEAN SLIDING WINDOW\n",
    "        for i in range(len(totalScores[actualSpeaker])):\n",
    "            ini = max(0,i-meanSideSize) # No negative values\n",
    "            end = i+meanSideSize+1 # +1 as python does not take into account last value\n",
    "            #print(ini,end)\n",
    "            totalScores[actualSpeaker][i] = np.mean(totalScores[actualSpeaker][ini:end])\n",
    "        print(totalScores[actualSpeaker])\n",
    "else:\n",
    "    # MINIMO\n",
    "    for actualSpeaker in res.keys():\n",
    "        print(\"SPEAKER\", actualSpeaker, \"LENGTH:\", len(res[actualSpeaker]))\n",
    "        for i in range(len(res[actualSpeaker])):\n",
    "            #center = i\n",
    "            center = faceFrames[actualSpeaker][0]+i\n",
    "            print(\"FRAME Nº \",center)\n",
    "            iAudio = tools.padAudio(audio,1,center,windowSize,videoFrames).unsqueeze(0)\n",
    "            iVideo = tools.padVideo(res[actualSpeaker],center-faceFrames[actualSpeaker][0],windowSize).unsqueeze(0)\n",
    "            #print(iAudio[0][0][0:5],iVideo[0][0][0][0:5])\n",
    "            scores,labels= model((iAudio,iVideo))\n",
    "            print(\"\\tProb Pos ->\",scores[sideWindowSize][1])\n",
    "            totalScores[actualSpeaker].append(scores[sideWindowSize][1].detach().cpu().numpy())\n",
    "        \n",
    "        # MEAN SLIDING WINDOW\n",
    "        for i in range(len(totalScores[actualSpeaker])):\n",
    "            ini = max(0,i-meanSideSize) # No negative values\n",
    "            end = i+meanSideSize+1 # +1 as python does not take into account last value\n",
    "            #print(ini,end)\n",
    "            totalScores[actualSpeaker][i] = np.mean(totalScores[actualSpeaker][ini:end])\n",
    "        print(totalScores[actualSpeaker])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "266\n",
      "271\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# SMOOTHING\n",
    "predArray = defaultdict(list)\n",
    "\n",
    "# APPLY THRESHOLD\n",
    "for actualSpeaker in res.keys():\n",
    "    # Delete < 0 frame predictions (initial)\n",
    "    #Solo para metodo secuencial ->\n",
    "    if sequential:\n",
    "        totalScores[actualSpeaker] = totalScores[actualSpeaker][sideWindowSize:len(res[actualSpeaker])+sideWindowSize]\n",
    "    # Classify as speaking if score higher than threshold\n",
    "    for sc in totalScores[actualSpeaker]:\n",
    "        predLabel = 1 if sc > thr else 0\n",
    "        predArray[actualSpeaker].append(predLabel)\n",
    "\n",
    "    print(len(predArray[actualSpeaker]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVideo(videoName,imgFrames,audioPath,width,height):\n",
    "    video = cv2.VideoWriter(\"output.mp4\", 0, 25, (width,height))\n",
    "\n",
    "    for image in imgFrames:\n",
    "        video.write(image)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    res = subprocess.check_output([\"ffmpeg\",\"-y\",\"-i\",os.getcwd()+f\"/output.mp4\",\"-i\",audioPath,\"-map\",\"0:v\",\"-map\",\"1:a\", \"-c:v\", \"copy\", \"-shortest\", os.getcwd()+f\"/outputs/videos/{videoName}\"])\n",
    "    os.remove(os.getcwd()+f\"/output.mp4\")\n",
    "    \n",
    "    print(res)\n",
    "\n",
    "# Returns a list of [start,end] timestamps in seconds of the parts where a speaker is speaking with minimum length of minLength frames\n",
    "def getSpeaking(arr,minLength,fps):\n",
    "    prev_idx = 0\n",
    "    posFrames = 0\n",
    "    idx_list = []\n",
    "    for i,num in enumerate(arr):\n",
    "        if num == 1:\n",
    "            posFrames +=1\n",
    "        else:\n",
    "            if i-prev_idx >= minLength:\n",
    "                idx_list.append((prev_idx/fps,i/fps))\n",
    "            prev_idx = i\n",
    "            posFrames = 0\n",
    "    return idx_list\n",
    "\n",
    "def saveFullVideo(videoName):\n",
    "    cap = cv2.VideoCapture(inputVideo)\n",
    "    videoImages = []\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    while True:\n",
    "        ret, image = cap.read()\n",
    "        videoImages.append(image)\n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "\n",
    "    for speakerN in totalScores.keys():\n",
    "        for i, fr in enumerate(faceFrames[speakerN]): #Para cada frame en el que aparezca la cara\n",
    "            try:\n",
    "                image = videoImages[fr]\n",
    "                greenValue = int(255*predArray[speakerN][i])\n",
    "                redValue = 255-greenValue\n",
    "                color = (0,greenValue,redValue)\n",
    "                #print(color)\n",
    "                xmin,ymin,xmax,ymax = facePos[speakerN][i]\n",
    "                image = cv2.rectangle(image, (xmin,ymin), (xmax,ymax),color , 1)\n",
    "                cv2.putText(image, \"{:.2f}\".format(totalScores[speakerN][i]*100), (xmin, ymin-5), cv2.FONT_HERSHEY_SIMPLEX, 0.35, color, 1)\n",
    "            except:\n",
    "                pass\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    createVideo(videoName,videoImages,audioPath,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "#Asignación de bounding boxes y probabilidades al video de output\n",
    "os.makedirs(\"outputs/videos\", exist_ok=True)\n",
    "path = os.path.normpath(inputVideo)\n",
    "videoName = str(path.split(os.sep)[-1])\n",
    "saveFullVideo(videoName)\n",
    "\n",
    "#print(predArray[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\whisper\\timing.py:42: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower median kernel implementation...\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\whisper\\timing.py:146: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower DTW implementation...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:08.340]  es que había una estrategia diseñada de realizar un golpe de Estado institucional al margen de la legalidad.\n",
      "[00:08.500 --> 00:10.140]  Y alguien deberá responder por ello.\n"
     ]
    }
   ],
   "source": [
    "# GUARDAR DATOS \n",
    "# COGER SOLO FRAGMENTO DE AUDIO EN EL QUE SE DETECTA HABLANTE\n",
    "os.makedirs(\"outputs/pickles\", exist_ok=True)\n",
    "model = whisper.load_model(\"small\")\n",
    "transcription = model.transcribe(audioPath, language=\"es\", verbose=True,word_timestamps=True)\n",
    "pkDict = {\"facePos\":facePos,\"faceFrames\":faceFrames,\"preds\":predArray,\"transcription\":transcription}\n",
    "with open(\"outputs/npz/\"+videoName+'.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(pkDict, f) # serialize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRAER PARTE APROPIADA DE LA TRANSCRIPCIÓN\n",
    "wordArr = []\n",
    "alignArr = []\n",
    "for seg in transcription[\"segments\"]:\n",
    "    for w in seg[\"words\"]:\n",
    "        wordArr.append(w[\"word\"])\n",
    "        alignArr.append((w[\"start\"],w[\"end\"]))\n",
    "print(wordArr)\n",
    "print(alignArr)\n",
    "pdRows = []\n",
    "\n",
    "for speakerN in totalScores.keys():\n",
    "    for (ini,end) in getSpeaking(predArray[speakerN],5,25):\n",
    "        iniW, endW = -1, -1\n",
    "        for i, (alignIni, alignEnd) in enumerate(alignArr):\n",
    "            if iniW == -1 and ini <= alignEnd and ini >= alignIni:\n",
    "                iniW = i\n",
    "            if endW == -1 and end <= alignEnd and end >= alignIni:\n",
    "                endW = i\n",
    "        if iniW > -1 and endW > -1:\n",
    "            newRow = {'video':inputVideo, 'speaker':speakerN, 'ini': ini, 'end':end, 'dataPath': \"outputs/npz/\"+videoName+\".pkl\", 'transcription':''.join(wordArr[iniW:endW+1])}\n",
    "            pdRows.append(newRow)\n",
    "\n",
    "df = pd.DataFrame(pdRows,columns=['video', 'speaker', 'ini', 'end', 'dataPath', 'transcription'])\n",
    "df.to_csv(r\"outputs/res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "df = pd.read_csv(\"testSamples.csv\")\n",
    "df2 = pd.DataFrame({'pred':totalPreds})\n",
    "df3 = pd.DataFrame({'posScore':totalScores})\n",
    "df[\"pred\"] = df2\n",
    "df[\"posScore\"] = df3\n",
    "df.to_csv(\"testPredsX.csv\")\n",
    "g_truth = pd.read_csv(\"testSamples.csv\")[\"label\"].values.tolist()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(g_truth, totalScores)\n",
    "auc = metrics.roc_auc_score(g_truth, totalScores)\n",
    "np.savetxt('cli.txt', df[df[\"label\"]==1][\"posScore\"].values, fmt='%f')\n",
    "np.savetxt('imp.txt', df[df[\"label\"]==0][\"posScore\"].values, fmt='%f')\n",
    "nCorrect = 0\n",
    "for i in df[df[\"label\"]==1][\"posScore\"].values:\n",
    "    if i >=0.5:\n",
    "        nCorrect +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    " return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "probs = df[\"posScore\"].values.tolist()\n",
    "test_y = df[\"label\"].values.tolist()\n",
    "\n",
    "scores = [f1_score(test_y, to_labels(probs, t)) for t in thresholds]\n",
    "ix = np.argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
