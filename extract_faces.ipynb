{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_detection\n",
    "from collections import defaultdict\n",
    "import os, python_speech_features\n",
    "import scipy.io.wavfile as wav\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "detector = face_detection.build_detector(\n",
    "  \"DSFDDetector\", confidence_threshold=.2, nms_iou_threshold=.2)\n",
    "pathVideo = \"C:/Users/jmmol/Desktop/LIP-RTVE/MP4s\"\n",
    "pathMFCC = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "pathAudio = \"C:/Users/jmmol/Desktop/LIP-RTVE/WAVs\"\n",
    "pathFaces = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolders(path):\n",
    "    \"\"\"\n",
    "    Crea una carpeta por cada hablante y dentro una por cada vídeo de ese hablante\n",
    "    También crea un diccionario que incluye los hablantes y cada uno de los nombres de sus vídeos y lo devuelve\n",
    "    Ej: speaker000/001\n",
    "    \"\"\"\n",
    "    speakerDict = defaultdict(list)\n",
    "    for (root,dirs,files) in os.walk(path, topdown=True):\n",
    "        #print (dirs)\n",
    "        for speaker in dirs:\n",
    "            os.makedirs(\"imgs/\"+speaker, exist_ok=True)\n",
    "            os.makedirs(\"mfccs/\"+speaker, exist_ok=True)\n",
    "            os.makedirs(\"npz/\"+speaker, exist_ok=True)\n",
    "        #print (files)\n",
    "        for f in files:\n",
    "            speaker, nmuestra = f.split(\"_\")\n",
    "            os.makedirs(\"imgs/\"+speaker+\"/\"+nmuestra.split(\".\")[0], exist_ok=True)\n",
    "            speakerDict[speaker].append(f[:-4]) #Quitamos .mp4\n",
    "    print ('Directorios creados')\n",
    "    return speakerDict\n",
    "\n",
    "def extractBiggestFace(img):\n",
    "    \"\"\"\n",
    "    Detecta todas las caras de una imagen y devuelve la más grande recortada y reescalada a 112x112\n",
    "    \"\"\"\n",
    "    detections = detector.detect(img)\n",
    "    idx_max = -1\n",
    "    area_max = -1\n",
    "    for i,cntr in enumerate(detections):\n",
    "        xmin,ymin,xmax,ymax = int(cntr[0]),int(cntr[1]),int(cntr[2]),int(cntr[3]) #Guardamos bounding box\n",
    "        area = (xmax-xmin)*(ymax-ymin)\n",
    "        if area > area_max: #Comprobamos si la cara es la más grande\n",
    "            idx_max = i\n",
    "            area_max = area\n",
    "            #print(area,idx_max)\n",
    "        #cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "\n",
    "    cntr = detections[idx_max]\n",
    "    try:\n",
    "        xmin,ymin,xmax,ymax = int(cntr[0]),int(cntr[1]),int(cntr[2]),int(cntr[3])\n",
    "        return cv2.resize(img[max(ymin,0):ymax, xmin:xmax], (112, 112)) #Cara detectada, reescalamos\n",
    "    except:\n",
    "        print(cntr)\n",
    "        cv2.imshow('image',img)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "def saveFaceCrops(videoPath):\n",
    "    speaker, videoID = videoPath.split(\"/\")[-2:]\n",
    "    videoID = videoID.split(\"_\")[1]\n",
    "    print(speaker, videoID)\n",
    "    vidcap = cv2.VideoCapture(videoPath)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    facesCount = 0\n",
    "    while success:\n",
    "        cv2.imwrite(\"imgs/\"+speaker+\"/\"+videoID+\"/\"+str(count)+'.jpg', extractBiggestFace(image)) #Detectamos y guardamos cara\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "    return count+1 #Devuelve número de frames\n",
    "\n",
    "def convertToNPZ(speakerDict, lengthsFilename):\n",
    "    \"\"\"\n",
    "    Convierte caras recortadas en npz en su directorio y devuelve longitudes de video\n",
    "    \"\"\"\n",
    "    videoLength = defaultdict(list)\n",
    "    for speaker in speakerDict:\n",
    "        for sample in speakerDict[speaker]:\n",
    "            folder = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/imgs/\"+speaker+\"/\"+sample.split(\"_\")[1]\n",
    "            images = []\n",
    "            #numFrames = len(os.listdir(folder))\n",
    "            for filename in sorted(os.listdir(folder),key=lambda x: int(x[:-4])):\n",
    "                img = cv2.imread(os.path.join(folder,filename))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "            numFrames = len(images)\n",
    "            videoLength[\"videos\"].append(sample)\n",
    "            videoLength[\"lengths\"].append(numFrames)\n",
    "            np.savez_compressed(r\"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz/\"+speaker+\"/\"+sample,images=images)\n",
    "    with open(lengthsFilename, 'wb') as f:\n",
    "        pickle.dump(videoLength, f)\n",
    "\n",
    "def extractMFCC(speakerDict,pathMFCC,pathAudio,pathNPZ):\n",
    "    for speaker in speakerDict:\n",
    "        for sample in speakerDict[speaker]:\n",
    "            _,sig = wav.read(pathAudio+\"/\"+speaker+\"/\"+sample+\".wav\")\n",
    "            videoRec = np.load(pathNPZ+\"/\"+speaker+\"/\"+sample+\".npz\")[\"images\"]\n",
    "            maxAudio = len(videoRec)*4 #Video a 25hz, audio a 100hz\n",
    "            audio = python_speech_features.mfcc(sig, 16000, numcep = 13, winlen = 0.025, winstep = 0.010)\n",
    "            if audio.shape[0] < maxAudio: #Si es un poco más corto hacemos padding\n",
    "                shortage = maxAudio - audio.shape[0]\n",
    "                audio = np.pad(audio, ((0, shortage), (0,0)), 'wrap')\n",
    "            audio = audio[:maxAudio,:] #Se recorta\n",
    "            np.savez_compressed(pathMFCC+\"/\"+speaker+\"/\"+sample,mfcc=audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorios creados\n"
     ]
    }
   ],
   "source": [
    "speakerDict = createFolders(pathVideo)\n",
    "#for speaker in sorted(list(speakerDict.keys())):\n",
    "#    for video in speakerDict[speaker]:\n",
    "#        saveFaceCrops(pathVideo+\"/\"+speaker+\"/\"+video)\n",
    "convertToNPZ(speakerDict,\"lengths.pickle\")\n",
    "#extractMFCC(speakerDict,pathMFCC,pathAudio,pathFaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fotos = np.load(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\npz\\speaker334\\speaker334_0000.npz\")[\"images\"]\n",
    "audio = np.load(r\"C:\\Users\\jmmol\\Desktop\\COSAS V7\\TFM\\mfccs\\speaker334\\speaker334_0000.npz\")[\"mfcc\"]\n",
    "print(len(fotos),len(audio))\n",
    "cv2.imshow(\"test\",fotos[-1])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSamples(splitSpeakers,nSamples,videoLengths,filename):\n",
    "    n = 0\n",
    "    sampleLabels = [\"positive\",\"partialMismatchSameSample\",\"partialMismatchDiffSample\",\"completeMismatch\"]\n",
    "    speakers = list(splitSpeakers.keys())\n",
    "    sampleList = []\n",
    "    repeatType = False\n",
    "    while n < nSamples: #TIENE QUE SER DISTINTOS VIDEOS\n",
    "        if not repeatType:\n",
    "            sType = np.random.choice(sampleLabels,p=[0.5,0.166666666,0.166666666,0.166666666])\n",
    "        speakerVideo = random.choice(speakers)\n",
    "        videoSample = random.choice(splitSpeakers[speakerVideo])\n",
    "        videoLength = videoLengths[videoSample]\n",
    "        center = random.randint(0,videoLength)\n",
    "        if sType == \"positive\":\n",
    "            audioSample = videoSample #AUDIO SAMPLE\n",
    "            #audioCenter = random.randint(0,videoLength*4)\n",
    "            sampleLabel = 1\n",
    "        if sType == \"partialMismatchSameSample\":\n",
    "            audioSample = videoSample\n",
    "            #audioCenter = random.randint(0,videoLengths[audioSample]*4)\n",
    "            sampleLabel = 0\n",
    "        if sType == \"partialMismatchDiffSample\":\n",
    "            speakerAudio = speakerVideo\n",
    "            audioSample = random.choice([s for s in splitSpeakers[speakerVideo] if s != videoSample])\n",
    "            if audioSample == videoSample:\n",
    "                print(\"NANI?\")\n",
    "            #audioCenter = random.randint(0,videoLengths[audioSample]*4)\n",
    "            sampleLabel = 0   \n",
    "        if sType == \"completeMismatch\":\n",
    "            speakerAudio = random.choice([s for s in speakers if s != speakerVideo])\n",
    "            audioSample = random.choice(splitSpeakers[speakerAudio])\n",
    "            #audioCenter = random.randint(0,videoLengths[audioSample]*4)\n",
    "            sampleLabel = 0\n",
    "\n",
    "        newRow = {'video':videoSample, 'audio':audioSample, 'label': sampleLabel, 'center': center}\n",
    "        if newRow not in sampleList:\n",
    "            sampleList.append(newRow)\n",
    "            repeatType = False \n",
    "            n+=1\n",
    "        else:\n",
    "            repeatType = True #Para poder balancear correctamente\n",
    "    df = pd.DataFrame(sampleList,columns=['video', 'audio', 'label', 'center'])\n",
    "    df.to_csv(filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitToDict(speakerList,lengths):\n",
    "    speakerDict = defaultdict(list)\n",
    "    for speaker in speakerList:\n",
    "        speakerDict[speaker[:-5]].append(speaker)\n",
    "    return speakerDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lengths.pickle', 'rb') as f: #Diccionario con clave de videos y clave de longitudes, ambas listas se corresponden\n",
    "    lengths = pickle.load(f)\n",
    "lengths = dict(zip(lengths[\"videos\"],lengths[\"lengths\"]))\n",
    "trainDF = pd.read_csv(\"C:/Users/jmmol/Desktop/LIP-RTVE/SPLITS/speaker-independent/train.csv\")\n",
    "trainSpeakers = splitToDict(trainDF[\"sampleID\"].values.tolist(),lengths)\n",
    "df = createSamples(trainSpeakers,100000,lengths,'trainSamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video</th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "      <th>center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8647</th>\n",
       "      <td>8647</td>\n",
       "      <td>speaker045_0074</td>\n",
       "      <td>speaker045_0074</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>25014</td>\n",
       "      <td>speaker009_0201</td>\n",
       "      <td>speaker009_0201</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30185</th>\n",
       "      <td>30185</td>\n",
       "      <td>speaker032_0004</td>\n",
       "      <td>speaker032_0004</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            video            audio  label  center\n",
       "8647         8647  speaker045_0074  speaker045_0074      0      35\n",
       "25014       25014  speaker009_0201  speaker009_0201      0      16\n",
       "30185       30185  speaker032_0004  speaker032_0004      0      90"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"trainSamples.csv\")\n",
    "df.loc[(df.label == 0) & (df.video == df.audio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video     16704\n",
      "audio     16704\n",
      "label     16704\n",
      "center    16704\n",
      "dtype: int64\n",
      "video     50035\n",
      "audio     50035\n",
      "label     50035\n",
      "center    50035\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.where((df.label == 0) & (df.video == df.audio)).count())\n",
    "print(df.where((df.label == 1)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
