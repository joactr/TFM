{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomNoOverlap(videoCenter, audioLen, treshold, nSideFrames):\n",
    "    \"\"\"\n",
    "    Threshold entre 0 y 1, porcentaje maximo de overlap permitido\n",
    "    No funciona para muestras muy pequeñas, echar ojo\n",
    "    \"\"\"\n",
    "    windowSize = nSideFrames*2+1\n",
    "    while True:\n",
    "        index = random.randint(0, audioLen)\n",
    "        overlap = False\n",
    "        if abs(videoCenter-index/4) < windowSize/(1/treshold):\n",
    "            overlap = True\n",
    "        if not overlap:\n",
    "            return index\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, nframes, video_dir, audio_dir, csv_path):\n",
    "        \"\"\"\n",
    "            nframes: descartamos videos que superen dicho nº de frames\n",
    "            video_dir: directorio donde se encuentran almacenados los videos\n",
    "            csv_path: fichero csv que define una partición\n",
    "        \"\"\"\n",
    "        self.nframes = nframes\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        samples_data = pd.read_csv(csv_path, delimiter=\",\")\n",
    "        #if \"train\" in csv_path:\n",
    "        #    samples_data = samples_data[samples_data[\"nFrames\"]< nframes]\n",
    "        \n",
    "        self.videoIDs = samples_data[\"video\"].tolist()\n",
    "        self.audioIDs = samples_data[\"audio\"].tolist()\n",
    "        self.labels = samples_data[\"label\"].tolist()\n",
    "        self.centers = samples_data[\"center\"].tolist()\n",
    "        self.nSideFrames = int((self.nframes-1)/2)\n",
    "        self.nSideFramesAudio = self.nSideFrames*4\n",
    "        print(self.nSideFrames,self.nSideFramesAudio)\n",
    "        #TESTING\n",
    "        self.ini = 0\n",
    "        self.fin = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videoIDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        videoID = self.videoIDs[index]\n",
    "        audioID = self.audioIDs[index]\n",
    "        label = self.labels[index]\n",
    "        center = self.centers[index]\n",
    "\n",
    "        video = self.__get_video__(videoID, audioID, label, center)\n",
    "        audio = self.__get_audio__(audioID, videoID, label, center)\n",
    "        if label == 1: #Positiva\n",
    "            label = np.ones(self.nframes)\n",
    "        else:\n",
    "            label = np.zeros(self.nframes)\n",
    "        label = torch.LongTensor(label)\n",
    "        return audio, video, label\n",
    "\n",
    "    def __get_video__(self, videoID, audioID, label, center):\n",
    "        spkrID = videoID.split(\"_\")[0]\n",
    "        video_path = os.path.join(self.video_dir, spkrID, videoID + \".npz\")\n",
    "        video = np.load(video_path)[\"images\"]\n",
    "        videoFrames = video.shape[0]\n",
    "        video = torch.FloatTensor(video)\n",
    "        ini = center-self.nSideFrames\n",
    "        fin = center+self.nSideFrames+1\n",
    "        #print(video.shape)\n",
    "        if center < self.nSideFrames: #Necesitamos hacer padding por la izquierda\n",
    "            padAmount = self.nSideFrames - center\n",
    "            video = F.pad(video,(0,0,0,0,padAmount,0), \"constant\", 0) #Padding al principio\n",
    "            ini = 0\n",
    "            fin = self.nframes\n",
    "            #print(\"pad izquierda:\",video.shape, padAmount)\n",
    "        if center+self.nSideFrames >= videoFrames: #Necesitamos hacer padding al final\n",
    "            padAmount = (self.nSideFrames+center) - videoFrames+1\n",
    "            video = F.pad(video,(0,0,0,0,0,padAmount), \"constant\", 0) #Padding al final\n",
    "            ini = len(video)-(self.nframes)\n",
    "            #print(\"pad derecha:\",video.shape, padAmount)\n",
    "        video = video[ini:fin]\n",
    "\n",
    "        # self.ini = ini\n",
    "        # self.fin = fin\n",
    "        # if(label ==1):\n",
    "        #     print(\"inifinVideo\",ini,fin)\n",
    "        # print(len(video))\n",
    "        # if len(video) != 51:\n",
    "        #    print(\"fake\",len(video))\n",
    "        return video # (T,96,96)\n",
    "    \n",
    "    def __get_audio__(self, audioID, videoID, label, center):\n",
    "        spkrID = audioID.split(\"_\")[0]\n",
    "        audio_path = os.path.join(self.audio_dir, spkrID, audioID + \".npz\")\n",
    "        audio = np.load(audio_path)[\"mfcc\"]\n",
    "        audio = torch.FloatTensor(audio)\n",
    "        audioFrames = audio.shape[0]\n",
    "        if label == 1: #Muestra positiva\n",
    "            center = center*4\n",
    "        if label == 0: #Muestra negativa\n",
    "            if audioID == videoID: #Audio desfasado\n",
    "                center = randomNoOverlap(center, audioFrames, 0.5, self.nSideFrames)\n",
    "            else:\n",
    "                center = random.randint(0,len(audio))\n",
    "        ini = center-self.nSideFramesAudio\n",
    "        fin = center+self.nSideFramesAudio+4\n",
    "        if center < self.nSideFramesAudio: #Necesitamos hacer padding por la izquierda\n",
    "            padAmount = self.nSideFramesAudio - center\n",
    "            audio = F.pad(audio,(0,0,padAmount,0), \"constant\", 0) #Padding al principio\n",
    "            ini = 0\n",
    "            fin = self.nframes*4\n",
    "            #print(\"pad izquierda audio:\",audio.shape, padAmount)\n",
    "        if center+self.nSideFramesAudio+4 >= audioFrames: #Necesitamos hacer padding al final\n",
    "            padAmount = (self.nSideFramesAudio+center) - audioFrames+4\n",
    "            audio = F.pad(audio,(0,0,0,padAmount), \"constant\", 0) #Padding al final\n",
    "            ini = len(audio)-(self.nframes*4)\n",
    "            #print(\"pad derecha audio:\",audio.shape, padAmount)\n",
    "            \n",
    "        audio = audio[ini:fin]\n",
    "        # if audio.shape[0] != 44:\n",
    "        #     print(\"center\",center,audioID==videoID)\n",
    "        #     print(\"fakeaudio:\",ini,fin,audio.shape[0])\n",
    "        #if (ini!=self.ini*4 or fin!=self.fin*4) and label == 1:\n",
    "        #    print(\"Error\")\n",
    "\n",
    "        return audio # (T,96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "dataset = MyDataset(10,videoDir,audioDir,\"trainSamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(100000)):\n",
    "    item = dataset.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,item[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"test\",item[0][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "item = dataset.__getitem__(0)\n",
    "img = np.asarray(item[1][0])\n",
    "print(item[2])\n",
    "#cv2.imshow(\"test\",img)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(item[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:\u001b[39m3\u001b[39m], [\u001b[39m0.299\u001b[39m, \u001b[39m0.587\u001b[39m, \u001b[39m0.114\u001b[39m])\n\u001b[0;32m      3\u001b[0m video\u001b[39m.\u001b[39mshape\n\u001b[1;32m----> 4\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,item[\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n"
     ]
    }
   ],
   "source": [
    "item = dataset.__getitem__(0)\n",
    "video = np.dot(item[1][0][...,:3], [0.299, 0.587, 0.114])\n",
    "video.shape\n",
    "cv2.imshow(\"test\",item[1][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset,batch_size=128,num_workers=0) #Cambiar num_workers\n",
    "dataloader_iterator = iter(dataloader)\n",
    "X = next(dataloader_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 13])\n",
      "torch.Size([5, 1, 20, 13])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,20,13)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "x = x.transpose(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
