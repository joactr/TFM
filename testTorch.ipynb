{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomNoOverlap(videoCenter, audioLen, treshold, nSideFrames):\n",
    "    \"\"\"\n",
    "    Threshold entre 0 y 1, porcentaje maximo de overlap permitido\n",
    "    No funciona para muestras muy pequeñas, echar ojo\n",
    "    \"\"\"\n",
    "    windowSize = nSideFrames*2+1\n",
    "    while True:\n",
    "        index = random.randint(0, audioLen)\n",
    "        overlap = False\n",
    "        if abs(videoCenter-index/4) < windowSize/(1/treshold):\n",
    "            overlap = True\n",
    "        if not overlap:\n",
    "            return index\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, nframes, video_dir, audio_dir, csv_path):\n",
    "        \"\"\"\n",
    "            nframes: descartamos videos que superen dicho nº de frames\n",
    "            video_dir: directorio donde se encuentran almacenados los videos\n",
    "            csv_path: fichero csv que define una partición\n",
    "        \"\"\"\n",
    "        self.nframes = nframes\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        samples_data = pd.read_csv(csv_path, delimiter=\",\")\n",
    "        #if \"train\" in csv_path:\n",
    "        #    samples_data = samples_data[samples_data[\"nFrames\"]< nframes]\n",
    "        \n",
    "        self.videoIDs = samples_data[\"video\"].tolist()\n",
    "        self.audioIDs = samples_data[\"audio\"].tolist()\n",
    "        self.labels = samples_data[\"label\"].tolist()\n",
    "        self.centers = samples_data[\"center\"].tolist()\n",
    "        self.nSideFrames = int((self.nframes-1)/2)\n",
    "        self.nSideFramesAudio = self.nSideFrames*4\n",
    "        print(self.nSideFrames,self.nSideFramesAudio)\n",
    "        #TESTING\n",
    "        self.ini = 0\n",
    "        self.fin = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videoIDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        videoID = self.videoIDs[index]\n",
    "        audioID = self.audioIDs[index]\n",
    "        label = self.labels[index]\n",
    "        center = self.centers[index]\n",
    "\n",
    "        video = self.__get_video__(videoID, audioID, label, center)\n",
    "        audio = self.__get_audio__(audioID, videoID, label, center)\n",
    "        \n",
    "\n",
    "        return video, audio, label\n",
    "\n",
    "    def __get_video__(self, videoID, audioID, label, center):\n",
    "        spkrID = videoID.split(\"_\")[0]\n",
    "        video_path = os.path.join(self.video_dir, spkrID, videoID + \".npz\")\n",
    "        video = np.load(video_path)[\"images\"]\n",
    "        videoFrames = video.shape[0]\n",
    "        video = torch.FloatTensor(video)\n",
    "        ini = center-self.nSideFrames\n",
    "        fin = center+self.nSideFrames+1\n",
    "        #print(video.shape)\n",
    "        if center < self.nSideFrames: #Necesitamos hacer padding por la izquierda\n",
    "            padAmount = self.nSideFrames - center\n",
    "            video = F.pad(video,(0,0,0,0,0,0,padAmount,0), \"constant\", 0) #Padding al principio\n",
    "            ini = 0\n",
    "            fin = self.nframes\n",
    "            #print(\"pad izquierda:\",video.shape, padAmount)\n",
    "        if center+self.nSideFrames >= videoFrames: #Necesitamos hacer padding al final\n",
    "            padAmount = (self.nSideFrames+center) - videoFrames+1\n",
    "            video = F.pad(video,(0,0,0,0,0,0,0,padAmount), \"constant\", 0) #Padding al final\n",
    "            ini = len(video)-(self.nframes)\n",
    "            #print(\"pad derecha:\",video.shape, padAmount)\n",
    "        video = video[ini:fin]\n",
    "        # self.ini = ini\n",
    "        # self.fin = fin\n",
    "        # if(label ==1):\n",
    "        #     print(\"inifinVideo\",ini,fin)\n",
    "        # print(len(video))\n",
    "        # if len(video) != 51:\n",
    "        #    print(\"fake\",len(video))\n",
    "        return video # (T,96,96)\n",
    "    \n",
    "    def __get_audio__(self, audioID, videoID, label, center):\n",
    "        spkrID = audioID.split(\"_\")[0]\n",
    "        audio_path = os.path.join(self.audio_dir, spkrID, audioID + \".npz\")\n",
    "        audio = np.load(audio_path)[\"mfcc\"]\n",
    "        audio = torch.FloatTensor(audio)\n",
    "        audioFrames = audio.shape[0]\n",
    "        if label == 1: #Muestra positiva\n",
    "            center = center*4\n",
    "        if label == 0: #Muestra negativa\n",
    "            if audioID == videoID: #Audio desfasado\n",
    "                center = randomNoOverlap(center, audioFrames, 0.5, self.nSideFrames)\n",
    "            else:\n",
    "                center = random.randint(0,len(audio))\n",
    "        ini = center-self.nSideFramesAudio\n",
    "        fin = center+self.nSideFramesAudio+4\n",
    "        if center < self.nSideFramesAudio: #Necesitamos hacer padding por la izquierda\n",
    "            padAmount = self.nSideFramesAudio - center\n",
    "            audio = F.pad(audio,(0,0,padAmount,0), \"constant\", 0) #Padding al principio\n",
    "            ini = 0\n",
    "            fin = self.nframes*4\n",
    "            #print(\"pad izquierda audio:\",audio.shape, padAmount)\n",
    "        if center+self.nSideFramesAudio+4 >= audioFrames: #Necesitamos hacer padding al final\n",
    "            padAmount = (self.nSideFramesAudio+center) - audioFrames+4\n",
    "            audio = F.pad(audio,(0,0,0,padAmount), \"constant\", 0) #Padding al final\n",
    "            ini = len(audio)-(self.nframes*4)\n",
    "            #print(\"pad derecha audio:\",audio.shape, padAmount)\n",
    "            \n",
    "        audio = audio[ini:fin]\n",
    "        # if audio.shape[0] != 44:\n",
    "        #     print(\"center\",center,audioID==videoID)\n",
    "        #     print(\"fakeaudio:\",ini,fin,audio.shape[0])\n",
    "        #if (ini!=self.ini*4 or fin!=self.fin*4) and label == 1:\n",
    "        #    print(\"Error\")\n",
    "\n",
    "        return audio # (T,96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8\n"
     ]
    }
   ],
   "source": [
    "videoDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/npz\"\n",
    "audioDir = \"C:/Users/jmmol/Desktop/COSAS V7/TFM/mfccs\"\n",
    "dataset = MyDataset(5,videoDir,audioDir,\"trainSamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 393/100000 [00:09<40:39, 40.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100000\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m     item \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(i)\n",
      "Cell \u001b[1;32mIn[24], line 51\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     48\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index]\n\u001b[0;32m     49\u001b[0m center \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenters[index]\n\u001b[1;32m---> 51\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_video__(videoID, audioID, label, center)\n\u001b[0;32m     52\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_audio__(audioID, videoID, label, center)\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m video, audio, label\n",
      "Cell \u001b[1;32mIn[24], line 60\u001b[0m, in \u001b[0;36mMyDataset.__get_video__\u001b[1;34m(self, videoID, audioID, label, center)\u001b[0m\n\u001b[0;32m     58\u001b[0m spkrID \u001b[39m=\u001b[39m videoID\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m video_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_dir, spkrID, videoID \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.npz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(video_path)[\u001b[39m\"\u001b[39;49m\u001b[39mimages\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     61\u001b[0m videoFrames \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     62\u001b[0m video \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(video)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\numpy\\lib\\npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[0;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[0;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs,\n\u001b[0;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_header_size)\n\u001b[0;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\numpy\\lib\\format.py:812\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    810\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[0;32m    811\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[1;32m--> 812\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39;49m\u001b[39marray data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    813\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    814\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[0;32m    816\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\site-packages\\numpy\\lib\\format.py:947\u001b[0m, in \u001b[0;36m_read_bytes\u001b[1;34m(fp, size, error_template)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[0;32m    944\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[0;32m    946\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 947\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(size \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[0;32m    948\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[0;32m    949\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\zipfile.py:924\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    923\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 924\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    925\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    926\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\jmmol\\miniconda3\\envs\\alc\\lib\\zipfile.py:1000\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m    999\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1000\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[0;32m   1001\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1004\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100000)):\n",
    "    item = dataset.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[275], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,item[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"test\",item[0][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset,batch_size=128,num_workers=0) #Cambiar num_workers\n",
    "dataloader_iterator = iter(dataloader)\n",
    "X = next(dataloader_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
